{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import extractor\n",
    "from feeder import VarFeeder\n",
    "import numba\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cached(name) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads csv file (maybe zipped) from data directory and caches it's content as a pickled DataFrame\n",
    "    :param name: file name without extension\n",
    "    :return: file content\n",
    "    \"\"\"\n",
    "    cached = 'data/%s.pkl' % name\n",
    "    sources = ['data/%s.csv' % name, 'data/%s.csv.zip' % name]\n",
    "    if os.path.exists(cached):\n",
    "        return pd.read_pickle(cached)\n",
    "    else:\n",
    "        for src in sources:\n",
    "            if os.path.exists(src):\n",
    "                df = pd.read_csv(src)\n",
    "                df.to_pickle(cached)\n",
    "                return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads source data for training/prediction\n",
    "    \"\"\"\n",
    "    def read_file(file):\n",
    "        df = read_cached(file).set_index('Page')\n",
    "        df.columns = df.columns.astype('M8[D]')\n",
    "        return df\n",
    "\n",
    "    # Path to cached data\n",
    "    path = os.path.join('data', 'all.pkl')\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_pickle(path)\n",
    "    else:\n",
    "        # Official data\n",
    "        df = read_file('train_2')\n",
    "        # Scraped data\n",
    "        scraped = read_file('2017-08-15_2017-09-11_new')\n",
    "        # Update last two days by scraped data\n",
    "        df[pd.Timestamp('2017-09-10')] = scraped['2017-09-10']\n",
    "        df[pd.Timestamp('2017-09-11')] = scraped['2017-09-11']\n",
    "\n",
    "        df = df.sort_index()\n",
    "        # Cache result\n",
    "        df.to_pickle(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:remove\n",
    "def make_holidays(tagged, start, end) -> pd.DataFrame:\n",
    "    def read_df(lang):\n",
    "        result = pd.read_pickle('data/holidays/%s.pkl' % lang)\n",
    "        return result[~result.dw].resample('D').size().rename(lang)\n",
    "\n",
    "    holidays = pd.DataFrame([read_df(lang) for lang in ['de', 'en', 'es', 'fr', 'ja', 'ru', 'zh']])\n",
    "    holidays = holidays.loc[:, start:end].fillna(0)\n",
    "    result =tagged[['country']].join(holidays, on='country').drop('country', axis=1).fillna(0).astype(np.int8)\n",
    "    result.columns = pd.DatetimeIndex(result.columns.values)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_x(start, end) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gets source data from start to end date. Any date can be None\n",
    "    \"\"\"\n",
    "    df = read_all()\n",
    "    # User GoogleAnalitycsRoman has really bad data with huge traffic spikes in all incarnations.\n",
    "    # Wikipedia banned him, we'll ban it too\n",
    "    bad_roman = df.index.str.startswith(\"User:GoogleAnalitycsRoman\")\n",
    "    df = df[~bad_roman]\n",
    "    if start and end:\n",
    "        return df.loc[:, start:end]\n",
    "    elif end:\n",
    "        return df.loc[:, :end]\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def single_autocorr(series, lag):\n",
    "    \"\"\"\n",
    "    Autocorrelation for single data series\n",
    "    :param series: traffic series\n",
    "    :param lag: lag, days\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s1 = series[lag:]\n",
    "    s2 = series[:-lag]\n",
    "    ms1 = np.mean(s1)\n",
    "    ms2 = np.mean(s2)\n",
    "    ds1 = s1 - ms1\n",
    "    ds2 = s2 - ms2\n",
    "    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n",
    "    return np.sum(ds1 * ds2) / divider if divider != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def batch_autocorr(data, lag, starts, ends, threshold, backoffset=0):\n",
    "    \"\"\"\n",
    "    Calculate autocorrelation for batch (many time series at once)\n",
    "    :param data: Time series, shape [n_pages, n_days]\n",
    "    :param lag: Autocorrelation lag\n",
    "    :param starts: Start index for each series\n",
    "    :param ends: End index for each series\n",
    "    :param threshold: Minimum support (ratio of time series length to lag) to calculate meaningful autocorrelation.\n",
    "    :param backoffset: Offset from the series end, days.\n",
    "    :return: autocorrelation, shape [n_series]. If series is too short (support less than threshold),\n",
    "    autocorrelation value is NaN\n",
    "    \"\"\"\n",
    "    n_series = data.shape[0]\n",
    "    n_days = data.shape[1]\n",
    "    max_end = n_days - backoffset\n",
    "    corr = np.empty(n_series, dtype=np.float64)\n",
    "    support = np.empty(n_series, dtype=np.float64)\n",
    "    for i in range(n_series):\n",
    "        series = data[i]\n",
    "        end = min(ends[i], max_end)\n",
    "        real_len = end - starts[i]\n",
    "        support[i] = real_len/lag\n",
    "        if support[i] > threshold:\n",
    "            series = series[starts[i]:end]\n",
    "            c_365 = single_autocorr(series, lag)\n",
    "            c_364 = single_autocorr(series, lag-1)\n",
    "            c_366 = single_autocorr(series, lag+1)\n",
    "            # Average value between exact lag and two nearest neighborhs for smoothness\n",
    "            corr[i] = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366\n",
    "        else:\n",
    "            corr[i] = np.NaN\n",
    "    return corr #, support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def find_start_end(data: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculates start and end of real traffic data. Start is an index of first non-zero, non-NaN value,\n",
    "     end is index of last non-zero, non-NaN value\n",
    "    :param data: Time series, shape [n_pages, n_days]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_pages = data.shape[0]\n",
    "    n_days = data.shape[1]\n",
    "    start_idx = np.full(n_pages, -1, dtype=np.int32)\n",
    "    end_idx = np.full(n_pages, -1, dtype=np.int32)\n",
    "    for page in range(n_pages):\n",
    "        # scan from start to the end\n",
    "        for day in range(n_days):\n",
    "            if not np.isnan(data[page, day]) and data[page, day] > 0:\n",
    "                start_idx[page] = day\n",
    "                break\n",
    "        # reverse scan, from end to start\n",
    "        for day in range(n_days - 1, -1, -1):\n",
    "            if not np.isnan(data[page, day]) and data[page, day] > 0:\n",
    "                end_idx[page] = day\n",
    "                break\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(start, end, valid_threshold) -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reads source data, calculates start and end of each series, drops bad series, calculates log1p(series)\n",
    "    :param start: start date of effective time interval, can be None to start from beginning\n",
    "    :param end: end date of effective time interval, can be None to return all data\n",
    "    :param valid_threshold: minimal ratio of series real length to entire (end-start) interval. Series dropped if\n",
    "    ratio is less than threshold\n",
    "    :return: tuple(log1p(series), nans, series start, series end)\n",
    "    \"\"\"\n",
    "    df = read_x(start, end)\n",
    "    starts, ends = find_start_end(df.values)\n",
    "    # boolean mask for bad (too short) series\n",
    "    page_mask = (ends - starts) / df.shape[1] < valid_threshold\n",
    "    print(\"Masked %d pages from %d\" % (page_mask.sum(), len(df)))\n",
    "    inv_mask = ~page_mask\n",
    "    df = df[inv_mask]\n",
    "    nans = pd.isnull(df)\n",
    "    return np.log1p(df.fillna(0)), nans, starts[inv_mask], ends[inv_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_indexes(begin, end) -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n",
    "    :param begin: start of date range\n",
    "    :param end: end of date range\n",
    "    :return: List of 4 Series, one for each lag. For each Series, index is date in range(begin, end), value is an index\n",
    "     of target (lagged) date in a same Series. If target date is out of (begin,end) range, index is -1\n",
    "    \"\"\"\n",
    "    dr = pd.date_range(begin, end)\n",
    "    # key is date, value is day index\n",
    "    base_index = pd.Series(np.arange(0, len(dr)), index=dr)\n",
    "\n",
    "    def lag(offset):\n",
    "        dates = dr - offset\n",
    "        return pd.Series(data=base_index.loc[dates].fillna(-1).astype(np.int16).values, index=dr)\n",
    "\n",
    "    return [lag(pd.DateOffset(months=m)) for m in (3, 6, 9, 12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_page_features(pages: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates page features (site, country, agent, etc) from urls\n",
    "    :param pages: Source urls\n",
    "    :return: DataFrame with features as columns and urls as index\n",
    "    \"\"\"\n",
    "    tagged = extractor.extract(pages).set_index('page')\n",
    "    # Drop useless features\n",
    "    features = tagged.drop(['term', 'marker'], axis=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_page_map(pages):\n",
    "    \"\"\"\n",
    "    Finds agent types (spider, desktop, mobile, all) for each unique url, i.e. groups pages by agents\n",
    "    :param pages: all urls (must be presorted)\n",
    "    :return: array[num_unique_urls, 4], where each column corresponds to agent type and each row corresponds to unique url.\n",
    "     Value is an index of page in source pages array. If agent is missing, value is -1\n",
    "    \"\"\"\n",
    "    import re\n",
    "    result = np.full([len(pages), 4], -1, dtype=np.int32)\n",
    "    pat = re.compile(\n",
    "        '(.+(?:(?:wikipedia\\.org)|(?:commons\\.wikimedia\\.org)|(?:www\\.mediawiki\\.org)))_([a-z_-]+?)')\n",
    "    prev_page = None\n",
    "    num_page = -1\n",
    "    agents = {'all-access_spider': 0, 'desktop_all-agents': 1, 'mobile-web_all-agents': 2, 'all-access_all-agents': 3}\n",
    "    for i, entity in enumerate(pages):\n",
    "        match = pat.fullmatch(entity)\n",
    "        assert match\n",
    "        page = match.group(1)\n",
    "        agent = match.group(2)\n",
    "        if page != prev_page:\n",
    "            prev_page = page\n",
    "            num_page += 1\n",
    "        result[num_page, agents[agent]] = i\n",
    "    return result[:num_page+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_page_features(df) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Applies one-hot encoding to page features and normalises result\n",
    "    :param df: page features DataFrame (one column per feature)\n",
    "    :return: dictionary feature_name:encoded_values. Encoded values is [n_pages,n_values] array\n",
    "    \"\"\"\n",
    "    def encode(column) -> pd.DataFrame:\n",
    "        one_hot = pd.get_dummies(df[column], drop_first=False)\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        return (one_hot - one_hot.mean()) / one_hot.std()\n",
    "\n",
    "    return {str(column): encode(column) for column in df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(values: np.ndarray):\n",
    "    return (values - values.mean()) / np.std(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Prepare data')\n",
    "# parser.add_argument('data_dir')\n",
    "# parser.add_argument('--valid_threshold', default=0.0, type=float, help=\"Series minimal length threshold (pct of data length)\")\n",
    "# parser.add_argument('--add_days', default=64, type=int, help=\"Add N days in a future for prediction\")\n",
    "# parser.add_argument('--start', help=\"Effective start date. Data before the start is dropped\")\n",
    "# parser.add_argument('--end', help=\"Effective end date. Data past the end is dropped\")\n",
    "# parser.add_argument('--corr_backoffset', default=0, type=int, help='Offset for correlation calculation')\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked 0 pages from 145036\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "df, nans, starts, ends = prepare_data(None, None, 0.0)\n",
    "# df, nans, starts, ends = prepare_data(args.start, args.end, args.valid_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare_dataメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_x(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145036, 804)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts, ends = find_start_end(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean mask for bad (too short) series\n",
    "valid_threshold = 0.0\n",
    "page_mask = (ends - starts) / df.shape[1] < valid_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mask = ~page_mask\n",
    "df = df[inv_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = pd.isnull(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read_allメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    df = read_cached(file).set_index('Page')\n",
    "    df.columns = df.columns.astype('M8[D]')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to cached data\n",
    "path = os.path.join('data', 'all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official data\n",
    "df = read_file('train_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped data\n",
    "scraped = read_file('2017-08-15_2017-09-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update last two days by scraped data\n",
    "df[pd.Timestamp('2017-09-10')] = scraped['2017-09-10']\n",
    "df[pd.Timestamp('2017-09-11')] = scraped['2017-09-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "# Cache result\n",
    "df.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-07-01', '2015-07-02', '2015-07-03', '2015-07-04',\n",
       "               '2015-07-05', '2015-07-06', '2015-07-07', '2015-07-08',\n",
       "               '2015-07-09', '2015-07-10',\n",
       "               ...\n",
       "               '2017-09-02', '2017-09-03', '2017-09-04', '2017-09-05',\n",
       "               '2017-09-06', '2017-09-07', '2017-09-08', '2017-09-09',\n",
       "               '2017-09-10', '2017-09-11'],\n",
       "              dtype='datetime64[ns]', length=804, freq=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User GoogleAnalitycsRoman has really bad data with huge traffic spikes in all incarnations.\n",
    "# Wikipedia banned him, we'll ban it too\n",
    "bad_roman = df.index.str.startswith(\"User:GoogleAnalitycsRoman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~bad_roman]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_start_endメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages = data.shape[0]\n",
    "n_days = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = np.full(n_pages, -1, dtype=np.int32)\n",
    "end_idx = np.full(n_pages, -1, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(n_pages):\n",
    "    # scan from start to the end\n",
    "    for day in range(n_days):\n",
    "        if not np.isnan(data[page, day]) and data[page, day] > 0:\n",
    "            start_idx[page] = day\n",
    "            break\n",
    "    # reverse scan, from end to start\n",
    "    for day in range(n_days - 1, -1, -1):\n",
    "        if not np.isnan(data[page, day]) and data[page, day] > 0:\n",
    "            end_idx[page] = day\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([803, 803, 803, ..., 803, 803, 803], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniq_page_mapメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.full([len(pages), 4], -1, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\n",
    "        '(.+(?:(?:wikipedia\\.org)|(?:commons\\.wikimedia\\.org)|(?:www\\.mediawiki\\.org)))_([a-z_-]+?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_page = None\n",
    "num_page = -1\n",
    "agents = {'all-access_spider': 0, 'desktop_all-agents': 1, 'mobile-web_all-agents': 2, 'all-access_all-agents': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prev_page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-abbdf47a301c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mprev_page\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprev_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mnum_page\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prev_page' is not defined"
     ]
    }
   ],
   "source": [
    "# for i, entity in enumerate(pages):\n",
    "#     match = pat.fullmatch(entity)\n",
    "#     assert match\n",
    "#     page = match.group(1)\n",
    "#     agent = match.group(2)\n",
    "#     if page != prev_page:\n",
    "#         prev_page = page\n",
    "#         num_page += 1\n",
    "#     result[num_page, agents[agent]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "entity = pages[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = pat.fullmatch(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = match.group(1)\n",
    "agent = match.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Awaken,_My_Love!\"_en.wikipedia.org'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_page = page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[num_page, agents[agent]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_autocorrメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lag, starts, ends, threshold, backoffset = df.values, 365, starts, ends, 1.5, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = data.shape[0]\n",
    "n_days = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_end = n_days - backoffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.empty(n_series, dtype=np.float64)\n",
    "support = np.empty(n_series, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.53878511e-316,   6.92698980e-310,   5.39497551e-317,\n",
       "         5.67930167e-311,   2.37151510e-322])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(n_series):\n",
    "#     series = data[i]\n",
    "#     end = min(ends[i], max_end)\n",
    "#     real_len = end - starts[i]\n",
    "#     support[i] = real_len/lag\n",
    "#     if support[i] > threshold:\n",
    "#         series = series[starts[i]:end]\n",
    "#         c_365 = single_autocorr(series, lag)\n",
    "#         c_364 = single_autocorr(series, lag-1)\n",
    "#         c_366 = single_autocorr(series, lag+1)\n",
    "#         # Average value between exact lag and two nearest neighborhs for smoothness\n",
    "#         corr[i] = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366\n",
    "#     else:\n",
    "#         corr[i] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = min(ends[i], max_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_len = end - starts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "support[i] = real_len/lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.20000000e+000,   2.19452055e+000,   6.92688373e-310, ...,\n",
       "         1.22655141e+266,   1.78495620e+161,   1.91084238e+214])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support[i] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = series[starts[i]:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_365 = single_autocorr(series, lag)\n",
    "c_364 = single_autocorr(series, lag-1)\n",
    "c_366 = single_autocorr(series, lag+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan nan\n"
     ]
    }
   ],
   "source": [
    "print(c_365, c_366, c_366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average value between exact lag and two nearest neighborhs for smoothness\n",
    "corr[i] = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([803, 803, 803, ..., 803, 803, 803], dtype=int32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lag_indexesメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin, end = df.columns[0], df.columns[-1] + pd.Timedelta(63, unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-07-01 00:00:00')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-11-13 00:00:00')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.date_range(begin, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = pd.Series(np.arange(0, len(dr)), index=dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(offset):\n",
    "    dates = dr - offset\n",
    "    return pd.Series(data=base_index.loc[dates].fillna(-1).astype(np.int16).values, index=dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lag(pd.DateOffset(months=m)) for m in (3, 6, 9, 12)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
